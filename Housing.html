<!DOCTYPE HTML>
<!--
	Zohaib Aftab, Data Scientist
	zohaibdr.github.io | 
	To showcase my projects
-->
<html>
	<head>
		<title>Project: Housing price conundrum </title>
		<meta charset="utf-8" />
		<meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no" />
		<!-- Bootstrap CSS -->
		<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap@4.4.1/dist/css/bootstrap.min.css" integrity="sha384-Vkoo8x4CGsO3+Hhxv8T/Q5PaXtkKtu6ug5TOeNV6gBiFeWPGFN9MuhOf23Q9Ifjh" crossorigin="anonymous">

		<link rel="stylesheet" href="assets/css/main.css" />
		<noscript><link rel="stylesheet" href="assets/css/noscript.css" /></noscript>

		<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css">

		<style>
			img{
				max-width: 90%;  
				display: block; /* remove extra space below image */
			}
		</style>

	</head>

	<body class="is-preload">

		<!-- Wrapper -->
			<div id="wrapper">

				<!-- Header -->
					<header id="header">
						<a href="index.html" class="logo">Home</a>
					</header>

				<!-- Nav -->
				<nav id="nav">
					<ul class="links">
						<li class="active"><a href="index.html">Projects</a></li>
						<li><a href="resume.html">Resum√©</a></li>
						<li><a href="Published.html">Published works</a></li>
					</ul>

					<ul class="icons">
						<li><a target="_blank" href="https://www.linkedin.com/in/drzohaib/" class="icon brands fa-linkedin"><span class="label">LinkedIn</span></a></li>
						<li><a target="_blank"  href=https://github.com/zohaibdr/DSportfolio_ class="icon brands fa-github"><span class="label">GitHub</span></a>
						</li>
						<li><a target="_blank"  href=mailto:zohaib@live.fr class="fa fa-envelope-open" style="font-size:22px"><span class="label"></span></a>
						</li>

					</ul>
				</nav>

				<!-- Main -->
					<div id="main">

						<!-- Post -->
		<section class="post">
			<header class="major">
				<h2> Housing Price Prediction </h2>
			</header>

			Tags: 
				One Hot encoding &bull; Data Wrangling &bull; Feature Creation  &bull; Feature scaling 
				<hr />
			<ul class="actions">
					<li> <a target = "_blank" href="https://github.com/zohaibdr/DSportfolio_" class="button primary">Python Code Here</a></li>
				</ul>
			<!-- <header> -->

				<h2>Context</h2>
				<p> Estimating a house price in an area is easy if you have the
				a large data set of historical prices against a myriad of features (square footage, locality, 
				number of rooms, etc.). 
				Trouble is  such data is not generally available to everybody. <br/>
				But what is available publically is the census data...with only a limited set attribute, relating to the locality/district. 

			</p>

				<h2>Problem Statement </h2>
				<p><b> Given the following set of attribute of the district, can we predict house price in the area accurately?  </b>  </p>
					<ol> 
						<li>Median income in the district </li>
						<li>Median house age </li>
						<li>Total number of rooms in the district </li>
						<li>Total number of bedrooms in the district</li>
						<li>District population,
						<li>Total district house occupancy</li>
						<li>District latitude</li>
						<li>District longitude</li>
						<li>Ocean proximity </li>
					</ol>
					
					<h2> Data set </h2>
						California housing data set containing 1990 census data <br/>
						
						Link to <a href="https://github.com/ageron/handson-ml2/tree/master/datasets/housing" target="_blank">Dataset</a> 
						<br/>
					
				</header>

				<header>
					<h2>Hypotheses</h2>
					<ol> <li> Median income of the district and its proximity to ocean must have a positive influence on the house price </li> 
							<li> Median house age should be correlated negatively to the median price  </li>
							<li> Some features must be highly correlated to each other such as total rooms vs. total bedrooms </li> 
							<li> Overall, we should be able to reasonably estimte the median house price, though not very accurately due to absence of key info as size of individual houses, crime rate, etc.  </li> 
																	</ol>
				</header>
				

				<hr />
				<!-- <header> -->
					<h1>Data Analysis</h1>

					
						An Exploratory data analysis reveals the distribution of data and the interrelation between various attributes. To avoid data leakage, I split the data into train/validation/test sets even before EDA. 
						<br> 
						In the data set, the features of 'total_bedrooms', 'population' and 'households' are highly correlated as indicated.
						This information allows reducing the number of features later on by eliminating some of the redundant features! <br>

					
					<p> <img src="images/housing/Correlation plot 1_edit.png" alt="" />  </p>
						
					Next, it is useful to know which features are strongly correlated with the <strong> Target variable </strong>. For this, I created the table below: 

					<p> <img src="images/housing/top5corr.png" alt="" /> </p>
					
					Turns out, the Median income is the single most important determinant of house price. Moreover, the fact of being away from the ocean (feature 'OceanIn') is negatively correlated, which make sense. <br>
					
					<h2> Introducing custom features </h2> 
					In order to improve the accuracy of machine learning models, a big part of modeling consists of creating new, more relevant features from raw data. For example, total number of rooms in a block are not useful attributes in determining house value unless we know the total housholds there are in that block. Similarly, population per house is more logical than total block population. <br>
					I introduced two features namely <strong> 'rooms_per_household' </strong> and <strong> 'population_per_household' </strong> and see their correlation with the target. 
<pre>
	<code> 
#create new variables first for new attributes 
NewAttrib1 = Xy_train_reduced['total_rooms']/Xy_train_reduced['households']
NewAttrib2 = Xy_train_reduced['population']/Xy_train_reduced['households'] 

#creating new variables allows inserting them at the specified location in the our </br> dataframe like here, just before last column (target variable) 
Xy_train_reduced.insert(loc = len(Xy_train_reduced.columns.values)-1, 
			column = 'rooms_per_household',   value=NewAttrib1)
Xy_train_reduced.insert(loc = len(Xy_train_reduced.columns.values)-1, 
			column = 'popolation_per_household', value=NewAttrib2)

	</code>
</pre>
It gives: 
										
<p>  <img src="images/housing/top5corrNew.png" alt="" />	 </p>
			Clearly, they have some correlation, if not very strong, to the house price. They will likely improve prediction accuracy. 

<h2> Scaling </h2>
Lastly, I scaled the data using 'StandardScaler' from sklearn library. <br>

<pre>
	<code>
from sklearn.preprocessing import StandardScaler

scaler = StandardScaler()
scaler.fit(X_train_reduced)         # fit only on training data

X_train_scaled = scaler.transform(X_train_reduced)

# Now apply same transformation to validation and test sets
X_test_scaled  = scaler.transform(X_test_reduced)  
X_valid_scaled  = scaler.transform(X_valid_reduced)  
	</code>
</pre>

<!-- <hr /> -->
		<h1>Results</h1>


		<h2>Establishing baseline </h2>
		It is best to first establish a baseline for prediction. I use linear regression as a baseline model in this case. <br>
		
		<br/> 
		<blockquote> 
		<strong> The cross validated R2 score on the training set is 0.62. The R2 score on test set is 0.63. </strong> <br> </blockquote>
		Below is the regression plot, for the test set. The baseline model predicts house value with a typical (RMSE) error of US$70k.

		<p> <img src="images/housing/regressPlot.png" alt="" />  </p> 
		
		<strong> The residual plot (left) shows that the model largely follows the assumption of homoscedasticity. The Q-Q plot on the right suggests that the assumptions of normality and linearity have also not been violated. 
</strong>
		<p> <img src="images/housing/output.png" alt="" /> </p>


		<h2> Improving predictions  </h2>										
		In order to improve upon the prediction accuracy, we could try to optimize hyper-parameters using the SGD regressor (scikit-learn). However, the strategy here is to first find the most powerful algorithm. <br>
		
		Turning to tree-based and ensemble methods, below are the train and test accuracies for <strong>5 popular algorithms</strong>: 

		<span > <img src="images/housing/treeR2_unoptim.png" alt="" /> </span>
		<blockquote> 
		<strong>All models are overfitting the data at default values which in Normal. </strong>
		</blockquote>
		

		<h2> Model Tuning </h2> 
		I tuned the models by adjusting their hyper-parameters using 'RandomSearchCV'. 
		
<pre>
	<code>
	from sklearn.model_selection import GridSearchCV, RandomizedSearchCV
	from sklearn.pipeline import Pipeline
	
	DecTree = DecisionTreeRegressor(random_state=10)
	RFreg = RandomForestRegressor(random_state=10)
	GBreg = GradientBoostingRegressor(random_state=10)
	XGBoostreg = XGBRegressor(random_state=10)
	
	param1 = {}     #Decision tree
	param1['regressor'] = [DecTree]
	param1['regressor__max_depth'] = np.arange(2,8)
	param1['regressor__min_samples_split']= np.arange(2, 8)
	param1['regressor__min_samples_leaf']= np.arange(5, 10)
	# param1['regressor__min_impurity_decrease'] = [0, 0.001, 0.002, 0.003]
	
	param2 = {}  #Random forests
	param2['regressor'] = [RFreg]
	param2['regressor__max_depth'] = np.arange(2,8) 
	param2['regressor__min_samples_split']= np.arange(2, 8)
	param2['regressor__min_samples_leaf']= np.arange(5, 10)
	param2['regressor__n_estimators'] = [10, 50, 100]
	
	param3 = {}  #Gradient Boost
	param3['regressor'] = [GBreg]
	param3['regressor__max_depth'] = np.arange(2,8) 
	param3['regressor__n_estimators'] = [50, 100, 200]  
	param3['regressor__min_samples_split']= np.arange(1, 1000, 100)
	param3['regressor__min_samples_leaf']= np.arange(1, 1000, 100)
	
	param4 = {} #XG Boost 
	param4['regressor'] = [XGBoostreg]
	param4['regressor__max_depth'] = np.arange(2,10) 
	
	pipeline = Pipeline([('regressor', DecTree)])
	params = [param1, param2, param3,  param4]
	
	randomSearch = RandomizedSearchCV(pipeline, params, scoring=None, cv=5, n_jobs= -1, verbose = 2, refit = True, return_train_score=True, n_iter = 20, random_state=10) 

	</code>
</pre>
		The resulting best model and optimal parameters were: <br> 
        <strong>GradientBoostingRegressor</strong> (max_depth=7, <br> 
                                           min_impurity_decrease=0.002, <br> 
                                           min_samples_leaf=200, <br> 
                                           min_samples_split=300, <br> 
                                           random_state=10))]) <br> 
										   <br>
<strong>The R2 score of the best estimator is: 0.750</strong> <br> 
<strong>The RMSE of the prediction is: 36563 dollars</strong>  <br> 
<p> 
<blockquote> 
<strong>Both of these scores are better than the baseline model (linear regression). </strong>
</blockquote>
</p>
	Finally, I calculated and plotted the features importances from the model:
	<span> <img src="images/housing/FeatImp.png" alt="" /> </span>

	As expected, the median income is the biggest determinant of housing price in the area as also indicated in the correlation plot. 

	
</div>

<hr/>

		<!-- Scripts -->
			<script src="assets/js/jquery.min.js"></script>
			<script src="assets/js/jquery.scrollex.min.js"></script>
			<script src="assets/js/jquery.scrolly.min.js"></script>
			<script src="assets/js/browser.min.js"></script>
			<script src="assets/js/breakpoints.min.js"></script>
			<script src="assets/js/util.js"></script>
			<script src="assets/js/main.js"></script>

	</body>
</html>
